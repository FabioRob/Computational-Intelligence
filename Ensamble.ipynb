{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensamble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j69wJSDkYMoL",
        "wZVzGuGxRCGf",
        "ENJxtLqlRWVc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j69wJSDkYMoL",
        "colab_type": "text"
      },
      "source": [
        "# Ensamble of Convolutional Neural Networks for classification of masses and calcifications in mammogram images\n",
        "\n",
        "Fabio D'Onofrio\n",
        "\n",
        "matricola : 556505\n",
        "\n",
        "Universit√† degli studi di Pisa\n",
        "\n",
        "Progetto del corso di Computational Intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gwRZG07SJzH",
        "colab_type": "text"
      },
      "source": [
        "This Notebook includes $two$ $sections$.\n",
        "\n",
        "The $first$ $section$ includes setting of the Drive directory in which mammogram images(both training set and test set) are stored, already preprocecced and patched, as numpy arrays with float values in the range 0-65535 and labels(0 corresponds to masses while 1 indicates calcification). In this section useful modules are also imported and some functions are defined in order to be used in the second section.\n",
        "\n",
        "In the $second$ $section$ the predictions of the best trained models are combined through a standard average and also with a weighted average with validation accuracies as weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZVzGuGxRCGf",
        "colab_type": "text"
      },
      "source": [
        "# SECTION 1 : Set the environment and download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTtQ8Q9lgvzo",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "5c5862d1-425a-4698-b220-8407c0470d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Set the working directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "data_dir = './drive/My Drive/Computational Intelligence 2019/FinalProject/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hEAGZrOg5ZV",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "fd7a3647-9b31-4b8a-9060-451fc2cd2759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Import modules\n",
        "#@markdown --- **os** for operating system dependent functionality\n",
        "import os\n",
        "#@markdown --- **json** to use JSON data-interchange format\n",
        "import json\n",
        "#@markdown --- **numpy** to handle multidimensional data\n",
        "import numpy as np\n",
        "#@markdown --- **matplotlib** for figures\n",
        "import matplotlib.pyplot as plt\n",
        "#@markdown --- **keras** deep learning library\n",
        "from   keras import models\n",
        "from   keras import layers\n",
        "from   keras.callbacks import EarlyStopping\n",
        "from   keras.callbacks import ModelCheckpoint\n",
        "from   keras.preprocessing.image import ImageDataGenerator\n",
        "from   keras.models import load_model\n",
        "from   keras import optimizers as opt\n",
        "#@markdown --- **random** to generate pseudo-random numbers\n",
        "import random as rn\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAeiTiFhBhl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Class data_setting \n",
        "class data_setting :\n",
        "#@markdown ---Member variables: **train_data, train_labels, test_data, test_labels** \n",
        "    def __init__(self,train_data,train_labels,test_data,test_labels):      \n",
        "    # Member variables initialization \n",
        "    # These four variables will be assigned to provided numpy arrays \n",
        "        self.train_data   = train_data\n",
        "        self.train_labels = train_labels\n",
        "        self.test_data    = test_data\n",
        "        self.test_labels  = test_labels\n",
        "#@markdown ---Method **shuffleDataset()** shuffles the data set\n",
        "    def shuffleDataset(self):\n",
        "    # This method shuffles the training data set   \n",
        "        dataset           = list(zip(self.train_data,self.train_labels))  \n",
        "      \n",
        "        rn.shuffle(dataset)\n",
        "        \n",
        "        self.train_data ,  self.train_label  = np.array(list(zip(*dataset)))\n",
        "#@markdown ---Method **data_manipulation()** reshapes and normalizes training and test sets\n",
        "    def data_manipulation(self) :\n",
        "    # This method shuffles,reshapes, and normalizes training and test sets\n",
        "        bits_per_pixel   = 16       \n",
        "\n",
        "        #self.shuffleDataset()\n",
        "\n",
        "        self.train_data = self.train_data.reshape(np.shape(self.train_data)+(1,))\n",
        "        self.train_data = self.train_data.astype('float32')/(2**bits_per_pixel-1)\n",
        "\n",
        "        self.test_data  = self.test_data.reshape(np.shape(self.test_data)+(1,))\n",
        "        self.test_data  = self.test_data.astype('float32')/(2**bits_per_pixel-1)\n",
        "                \n",
        "##############################################################################\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qy1gBsYhJhF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Other functions used to search for the best model\n",
        "\n",
        "#@markdown --- **build_dict()** loads all previous trained CNN's dictionaries and stores them in a single variable    \n",
        "def build_dict(models_dir,models_folder_name) :\n",
        "# This method iterates over all folders containing models_folder_name in their names, in the working directory \n",
        "# and returns a dictionary containing all models dictionaries found\n",
        "  models_dictionaries = {}\n",
        "\n",
        "  for models_folder in os.listdir(models_dir) : \n",
        "\n",
        "    if models_folder_name in models_folder :\n",
        "\n",
        "      for files in os.listdir(models_dir + '/' + models_folder) :\n",
        "\n",
        "        if 'Model_dict_' in files :\n",
        "\n",
        "          mod_dict = json.load(open(models_dir + '/' + models_folder + '/' + files )) \n",
        "\n",
        "          mod_key = 'mod_'+models_folder[models_folder.find('_')+1:] + '_' + files[files.find('_')+1:]\n",
        "          models_dictionaries[mod_key] = mod_dict\n",
        "          model_number = files[files.find('t')+2:]\n",
        "          model_path = models_dir + models_folder + '/'\n",
        "          models_dictionaries[mod_key]['path'] = { 'model_path' : model_path , 'model_number' : model_number }\n",
        "          \n",
        "  return models_dictionaries\n",
        "#@markdown ---  **best_acc_model()** returns the best trained CNNs\n",
        "def best_acc_model(models_dictionaries , n ) :\n",
        "\n",
        "  \"\"\"\n",
        "  This function finds the model with the best n test accuracy and outputs the respective dictionary\n",
        "  \"\"\"\n",
        "  test_dictionaries = {}\n",
        "  for mods in list(models_dictionaries.keys()) :\n",
        "    test_dictionaries[mods]=models_dictionaries[mods]['mod_history']['test_acc']\n",
        "\n",
        "  sorted_by_value = sorted(test_dictionaries.items(), key=lambda x: x[1])\n",
        "  best_n_models = [k[0] for k in sorted_by_value[-n:]]\n",
        "\n",
        "  return [models_dictionaries[i] for i in best_n_models ]\n",
        "\n",
        "#@markdown --- **load_best_models()** loads the n best CNNs\n",
        "def load_best_models(folder , n) :\n",
        "    \n",
        "    best_models_dir = data_dir + folder + '/'\n",
        "    best_models_h5 = []\n",
        "    models_dict = build_dict(data_dir,folder)\n",
        "    best_models_dict = best_acc_model(models_dict , n )\n",
        "    \n",
        "    for mod in best_models_dict :\n",
        "        mod_number = mod['path']['model_number']\n",
        "        best_models_h5.append(load_model(best_models_dir+'model_'+str(mod_number)+'.h5'))\n",
        "    \n",
        "    return best_models_h5\n",
        "\n",
        "##############################################################################  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBzgY_Y-hNFF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download data\n",
        "############################################################################################################################################################\n",
        "#@markdown --- **train_images_150** numpy array of shape (training batch size,width,height)=(2864,150,150)\n",
        "train_images_150=np.load(data_dir + 'train_img_150.npy')        # 150x150 pixels training images with values in [0,2^16 -1]\n",
        "#@markdown --- **train_labels** numpy array of shape (batch size,)=(2864,)\n",
        "train_labels=np.load(data_dir + 'train_lab.npy')                # training labels\n",
        "#@markdown --- **test_images_150** numpy array of shape (test set size,width,height)=(352,150,150)\n",
        "test_images_150=np.load(data_dir + 'public_test_image_150.npy') # 150x150 pixels test images  with values in [0,2^16 -1]\n",
        "#@markdown --- **test_labels** numpy array of shape (test set size,)=(352,)\n",
        "test_labels=np.load(data_dir + 'public_test_label.npy')         # test labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJxtLqlRWVc",
        "colab_type": "text"
      },
      "source": [
        "# SECTION 2 : Ensamble of Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUsSiacgcpFk",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c8a12759-f3b6-4075-98ea-5773d37285da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#@title Ensamble network will be composed of some of the best trained from scratch CNNs and the pre-trained CNN \n",
        "\n",
        "data_set = data_setting(train_images_150,train_labels,test_images_150,test_labels)\n",
        "data_set.data_manipulation()\n",
        "#@markdown Set the name of the folders in which best trained from scratch models have been stored\n",
        "best_models_folder_name = 'Best_TrainedModels' #@param {type:\"string\"}\n",
        "\n",
        "models_dictionaries = build_dict(data_dir,best_models_folder_name)\n",
        "\n",
        "#@markdown Set how many of the best models trained from scratch must be used in the Ensemble Network\n",
        "n_bests = 3 #@param {type : \"integer\" }\n",
        "\n",
        "best_models_dict = best_acc_model(models_dictionaries , n_bests ) \n",
        "\n",
        "best_models_h5 = load_best_models(best_models_folder_name , n_bests)\n",
        "\n",
        "#@markdown Set the name of the folder in which the best pre-trained CNN has been saved\n",
        "pt_folder = 'final_PT' #@param {type: \"string\" }\n",
        "\n",
        "pt_model = load_model(data_dir+pt_folder+'/best_model.h5')\n",
        "\n",
        "#@markdown Set the name of the folder in which the CNN models of the Ensamble Network will be stored\n",
        "ensamble_folder = 'Ensamble' #@param {type : \"string\" }\n",
        "if not os.path.exists(data_dir+ensamble_folder+'/') :\n",
        "  os.mkdir(data_dir+ensamble_folder+'/')\n",
        "pt_model.save(data_dir+ensamble_folder+'/PT_model.h5')\n",
        "pt_model_dict = json.load(open(data_dir+pt_folder+'/Model_dict')) \n",
        "three_channels_test_data=data_set.test_data*np.ones((len(data_set.test_data),1,1,3))\n",
        "pt_predictions = pt_model.predict(three_channels_test_data)\n",
        "\n",
        "models_predictions = []\n",
        "i=1\n",
        "for mods in best_models_h5 : \n",
        "  mods.save(data_dir+ensamble_folder+'/Scracth_Model_'+str(i)+'.h5')\n",
        "  models_predictions.append(mods.predict(data_set.test_data))\n",
        "  i += 1\n",
        "\n",
        "models_predictions.append(pt_predictions)\n",
        "best_models_dict.append(pt_model_dict)\n",
        "\n",
        "weighted_average_prediction = 0\n",
        "best_models_val_acc = []\n",
        "for i in range(n_bests+1) :\n",
        "    last_val_acc = best_models_dict[i]['mod_history']['val_acc'][-1]\n",
        "    best_models_val_acc.append(last_val_acc)\n",
        "    weighted_average_prediction += models_predictions[i]*last_val_acc\n",
        "    \n",
        "weighted_average_prediction = weighted_average_prediction/sum(best_models_val_acc)\n",
        "weighted_loss = log_loss(test_labels,weighted_average_prediction)\n",
        "print('Binary Cross Entropy loss of the weighted average predictions is : ' , weighted_loss)\n",
        "\n",
        "weighted_average_prediction = [ round(float(b)) for b in weighted_average_prediction]    \n",
        "correct_avg_predictions = np.sum(np.equal(weighted_average_prediction,test_labels))\n",
        "weighted_accuracy = correct_avg_predictions/len(test_labels)\n",
        "print('Weighted average accuracy is : ' , weighted_accuracy)\n",
        "    \n",
        "average_prediction=[]   \n",
        "for i in range(len(test_labels)) :\n",
        "    prediction = 0\n",
        "    for j in range(n_bests+1) :\n",
        "        prediction += models_predictions[j][i]\n",
        "    average_prediction.append(prediction/(n_bests+1))\n",
        "\n",
        "average_loss = log_loss(test_labels,average_prediction)\n",
        "print('Binary Cross Entropy loss of the average predictions is : ' , average_loss)\n",
        "\n",
        "average_prediction = [ round(float(b)) for b in average_prediction]    \n",
        "correct_predictions = np.sum(np.equal(average_prediction,test_labels))\n",
        "accuracy = correct_predictions/len(test_labels)\n",
        "print('Average accuracy is : ' ,accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary Cross Entropy loss of the weighted average predictions is :  0.2636623165245427\n",
            "Weighted average accuracy is :  0.9034090909090909\n",
            "Binary Cross Entropy loss of the average predictions is :  0.26416203314593906\n",
            "Average accuracy is :  0.9005681818181818\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}